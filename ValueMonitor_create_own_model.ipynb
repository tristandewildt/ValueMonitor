{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ValueMonitor - Create your own topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page is a visualisation of the ValueMonitor prototype. In case you would like to use the notebook, click on the icon ‘**Run in Google Colab**’ hereunder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tristandewildt/ValueMonitor/blob/main/ValueMonitor_create_own_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tristandewildt/ValueMonitor/blob/main/ValueMonitor_create_own_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content:\n",
    "* [1. Import dataset and packages](#import_dataset_and_packages)\n",
    "* [2. Creating the topic model](#creating_the_topic_model)\n",
    "* [3. Verifying the topic model](#verifying_the_topic_model)\n",
    "* [4. Gap assessment](#gap_assessment)\n",
    "* [5. Impact assessment](#impact_assessment)\n",
    "* [6. Values in different societal_groups](#values_in_different_societal_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dataset and packages  <a name=\"import_dataset_and_packages\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the dataset and relavant python packages are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "''' Packages'''\n",
    "\n",
    "!pip install corextopic\n",
    "!pip install joblib\n",
    "!pip install tabulate\n",
    "!pip install simple_colors\n",
    "!pip install ipyfilechooser\n",
    "\n",
    "import os, sys, importlib\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, Button\n",
    "import pickle\n",
    "from ipyfilechooser import FileChooser\n",
    "from tkinter import Tk, filedialog\n",
    "from IPython.display import clear_output, display\n",
    "from google.colab import files\n",
    "import nltk\n",
    "import io\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "''' Source code'''\n",
    "\n",
    "user = \"tristandewildt\"\n",
    "repo = \"ValueMonitor\"\n",
    "src_dir = \"code\"\n",
    "pyfile_1 = \"make_topic_model.py\"\n",
    "pyfile_2 = \"create_visualisation.py\"\n",
    "\n",
    "if os.path.isdir(repo):\n",
    "    !rm -rf {repo}\n",
    "\n",
    "!git clone https://github.com/{user}/{repo}.git\n",
    "\n",
    "path = f\"{repo}/{src_dir}\"\n",
    "if not path in sys.path:\n",
    "    sys.path.insert(1, path)\n",
    "\n",
    "make_topic_model = importlib.import_module(pyfile_1.rstrip(\".py\"))\n",
    "create_visualisation = importlib.import_module(pyfile_2.rstrip(\".py\"))\n",
    "\n",
    "from make_topic_model import *\n",
    "from create_visualisation import *\n",
    "\n",
    "''' Datasets'''\n",
    "\n",
    "#!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=12ZyryF8MbMYKuhIBEhUUvnvx43_cna56' -O dataset_ValueMonitor_prototype\n",
    "\n",
    "#with open('dataset_ValueMonitor_prototype', \"rb\") as fh:\n",
    "#    df = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import dateutil.parser\n",
    "import matplotlib.pyplot as plt\n",
    "from corextopic import vis_topic as vt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from simple_colors import *\n",
    "from ipywidgets import Button\n",
    "from tkinter import Tk, filedialog\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#plt.use( 'tkagg' )\n",
    "\n",
    "sys.path.append('./code')\n",
    "\n",
    "#from convert_to_df import *\n",
    "from make_topic_model import *\n",
    "from create_visualisation import *\n",
    "#from import_topic_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-30cbd90b3e23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "csv = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = io.BytesIO(csv[list(df.keys())[0]])\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT DrDJNowakowski Fuels value added chemicals ...</td>\n",
       "      <td>2019-03-30 22:30:01+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crude awakening commodity traders look oil FT ...</td>\n",
       "      <td>2019-03-30 22:29:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recent conference Seattle Understanding Opport...</td>\n",
       "      <td>2019-03-30 22:24:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT DSM bioeconomy goes biofuels biomaterials i...</td>\n",
       "      <td>2019-03-30 21:33:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bioeconomy goes biofuels biomaterials is safe ...</td>\n",
       "      <td>2019-03-30 21:32:25+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Visit t co VMUEqAEgXc Biofuel experts have bee...</td>\n",
       "      <td>2019-03-28 14:29:48+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Visit t co VMUEqAEgXc Don t miss opportunity a...</td>\n",
       "      <td>2019-03-28 14:29:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Visit t co YnTmnT3Ps8 Submit abstract book slo...</td>\n",
       "      <td>2019-03-28 14:29:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>PowerPoint hiring expert witness biofuels bioc...</td>\n",
       "      <td>2019-03-28 14:24:02+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>EdKrassen realDonaldTrump AOC Let s spend tril...</td>\n",
       "      <td>2019-03-28 14:21:31+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    RT DrDJNowakowski Fuels value added chemicals ...   \n",
       "1    Crude awakening commodity traders look oil FT ...   \n",
       "2    recent conference Seattle Understanding Opport...   \n",
       "3    RT DSM bioeconomy goes biofuels biomaterials i...   \n",
       "4    bioeconomy goes biofuels biomaterials is safe ...   \n",
       "..                                                 ...   \n",
       "475  Visit t co VMUEqAEgXc Biofuel experts have bee...   \n",
       "476  Visit t co VMUEqAEgXc Don t miss opportunity a...   \n",
       "477  Visit t co YnTmnT3Ps8 Submit abstract book slo...   \n",
       "478  PowerPoint hiring expert witness biofuels bioc...   \n",
       "479  EdKrassen realDonaldTrump AOC Let s spend tril...   \n",
       "\n",
       "                         date  \n",
       "0   2019-03-30 22:30:01+00:00  \n",
       "1   2019-03-30 22:29:05+00:00  \n",
       "2   2019-03-30 22:24:00+00:00  \n",
       "3   2019-03-30 21:33:07+00:00  \n",
       "4   2019-03-30 21:32:25+00:00  \n",
       "..                        ...  \n",
       "475 2019-03-28 14:29:48+00:00  \n",
       "476 2019-03-28 14:29:30+00:00  \n",
       "477 2019-03-28 14:29:05+00:00  \n",
       "478 2019-03-28 14:24:02+00:00  \n",
       "479 2019-03-28 14:21:31+00:00  \n",
       "\n",
       "[480 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_select_as_text = [\"text\"]\n",
    "column_as_date = [\"date\"]\n",
    "other_columns_to_keep = []\n",
    "\n",
    "wordtagging = True # True, False\n",
    "tags_to_select = ['NN', 'NNP', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ']\n",
    "# https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "df = clean_df(df, columns_to_select_as_text, column_as_date, other_columns_to_keep, wordtagging, tags_to_select)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqtK4E3CSnIS"
   },
   "source": [
    "## 2. Creating the topic model <a name=\"creating_the_topic_model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrZc5SEz70E6"
   },
   "source": [
    "In this step, we create a topic model in which some of the topics refer to values. The creation of topics that reflect values is done by means of so-called 'anchor' words. These words guide the algorithm in the creation of topics that reflect values.\n",
    "\n",
    "Anchor words are typically words that people use to refer to (the idea of) a value, such as synonyms. After adding some anchor words and running the model, the algorithm will automatically pick up other words that refer to the value. This is because the algorithm has observed that these words are often mentionned in the same documents as the anchor words.\n",
    "\n",
    "Finding the right anchor words is typically an iterative process, by observing the new topic model created by the algorithm. Some anchor words need to be added to ensure that some aspect of the value are not left behind (to be placed in *dict_anchor_words* in the cell below). Other words need to be removed since they do not refer to the value (in *list_rejected_words* in the cell below).\n",
    "\n",
    "We have prefilled an number of anchor words for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_anchor_words = {\n",
    "\"Justice and Fairness\" : [\"justice\", \"fairness\", \"fair\", \"equality\", \"unfair\"],\n",
    "\"Privacy\" : [\"privacy\", \"personal data\", \"personal sphere\", \"data privacy\", \"privacy protection\", \"privacy concerns\", \n",
    "             \"confidentiality\"],\n",
    "\"Cyber-security\" : [\"cyber\", \"security\", \"cybersecurity\", \"malicious\", \"attacks\"],\n",
    "\"Environmnental Sustainability\" : [\"sustainability\", \"sustainable\", \"renewable\", \"durable\", \"durability\",\n",
    "                                  \"sustainable development\", \"environmental\"],\n",
    "\"Transparency\" : [\"transparency\", \"transparent\", \"transparently\", \"explainability\", \"interpretability\", \"explainable\",\n",
    "                 \"opaque\", \"interpretable\"],\n",
    "\"Accountability\" : [\"accountable\", \"accountability\", \"accountable\", \"traceability\", \"traceable\"],\n",
    "\"Autonomy\" : [\"autonomy\", \"self-determination\", \"autonomy human\", \"personal autonomy\"], \n",
    "\"Democracy\" : [\"democracy\", \"democratic\", \"human rights\", \"freedom speech\", \"equal representation\",\n",
    "              \"political\"], \n",
    "\"Reliability\" : [\"reliability\", \"reliable\", \"robustness\", \"robust\", \"predictability\"],\n",
    "\"Trust\" : [\"trust\", \"trustworthy\", \"trustworthiness\", \"confidence\", \"honesty\"],\n",
    "\"Well-being\" : [\"well being\", \"well-being\", \"wellbeing\", \"quality life\",\n",
    "               \"good life\", \"qol\", \"life satisfaction\", \"welfare\"],\n",
    "\"Inclusiveness\" : [\"inclusiveness\", \"inclusive\", \"inclusivity\", \"discrimination\", \"diversity\"]\n",
    "}\n",
    "\n",
    "list_rejected_words = [\"iop\", \"iop publishing\", \"publishing ltd\", \"publishing\", \"licence iop\",\n",
    "                       \"mdpi basel\", \"basel switzerland\", \"mdpi\", \"basel\", \"licensee mdpi\", \"licensee\", \"authors licensee\", \n",
    "                       \"switzerland\", \"authors\", \"publishing limited\", \"emerald\", \"emerald publishing\", ]\n",
    "\n",
    "list_anchor_words_other_topics = [\n",
    "        [\"internet of things\", \"iot\", \"internet things\", \"iot devices\", \"things iot\"],\n",
    "        [\"artificial intelligence\", \"ai\", \"artificial\"],\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203570,
     "status": "ok",
     "timestamp": 1626379422615,
     "user": {
      "displayName": "Tristan de Wildt",
      "photoUrl": "",
      "userId": "12635936161789443610"
     },
     "user_tz": -120
    },
    "id": "pB9tQrBBYxd3",
    "outputId": "6e4f793e-5fd9-4566-e79e-6a005584f5d3",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_tagged'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_tagged'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e0da20716c12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;34m'''--------------------------------------------------------------------------'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel_and_vectorized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_anchored_topic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_topics_to_find\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_documents_in_analysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_anchor_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_anchor_words_other_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_rejected_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreport_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_and_vectorized_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_anchor_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumber_of_words_per_topic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdf_with_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_df_with_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_and_vectorized_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_and_vectorized_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_topics_to_find\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\SURFdrive\\ValueMonitor_Workshop\\code\\make_topic_model.py\u001b[0m in \u001b[0;36mmake_anchored_topic_model\u001b[1;34m(df, number_of_topics, number_of_documents_in_analysis, dict_anchor_words, list_anchor_words_other_topics, list_rejected_words)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_anchored_topic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_documents_in_analysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_anchor_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_anchor_words_other_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_rejected_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;34m''' Think here what could be some errors that people could make with regard to input data '''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[0mdf_reduced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_documents_in_analysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\SURFdrive\\ValueMonitor_Workshop\\code\\make_topic_model.py\u001b[0m in \u001b[0;36mvectorize\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     )\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_tagged'"
     ]
    }
   ],
   "source": [
    "number_of_topics_to_find = 100\n",
    "number_of_documents_in_analysis = 2000\n",
    "\n",
    "number_of_words_per_topic_to_show = 10\n",
    "number_of_words_per_topic = 10\n",
    "\n",
    "'''--------------------------------------------------------------------------''' \n",
    "\n",
    "model_and_vectorized_data = make_anchored_topic_model(df, number_of_topics_to_find, min(number_of_documents_in_analysis, len(df)), dict_anchor_words, list_anchor_words_other_topics, list_rejected_words)\n",
    "topics = report_topics(model_and_vectorized_data[0], dict_anchor_words,number_of_words_per_topic)\n",
    "df_with_topics = create_df_with_topics(df, model_and_vectorized_data[0], model_and_vectorized_data[1], number_of_topics_to_find)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verifying the topic model   <a name=\"verifying_the_topic_model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify whether topics sufficiently refer to values, the code hereunder can be used to evaluate whether documents indeed address the value in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26006cf3fc04f7db819f1d698cc496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='selected_value', options=('Justice and Fairness', 'Privacy', 'Cybe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_print_sample_articles_topic(selected_value, size_sample)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_print_sample_articles_topic(selected_value, size_sample):\n",
    "\n",
    "    show_extracts = True # True, False\n",
    "    show_full_text  = False # True, False\n",
    "\n",
    "    print_sample_articles_topic(df_with_topics, dict_anchor_words, topics, selected_value, size_sample, show_extracts, show_full_text)\n",
    "\n",
    "interact(plot_print_sample_articles_topic, selected_value=[*dict_anchor_words], size_sample =(5,50, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gap assessment <a name=\"gap_assessment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes time before a good topic model is build in which topics adequately represent values. The code in the next cell can be used to import an existing topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a005468eb3794666adc1109e988a5d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Selected_technology', options=('AI', 'IoT'), value='AI'), Output()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_values_in_different_datasets(Selected_technology)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_values_in_different_datasets(Selected_technology):\n",
    "    values_in_different_datasets(df_with_topics, Selected_technology, dict_anchor_words)\n",
    "\n",
    "interact(plot_values_in_different_datasets, Selected_technology=[\"AI\", \"IoT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac63b1a43d74555a9b7503e504e52c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='selected_technology', options=('AI', 'IoT'), value='AI'), Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_print_sample_articles_topic(selected_technology, selected_value, selected_dataset, size_sample)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_print_sample_articles_topic(selected_technology, selected_value, selected_dataset, size_sample):\n",
    "    show_extracts = True # True, False\n",
    "    show_full_text  = False # True, False\n",
    "    df_with_topics_selected_technology = df_with_topics[df_with_topics[selected_technology] == True]\n",
    "    df_with_topics_selected_technology_dataset = df_with_topics_selected_technology[df_with_topics_selected_technology['dataset'] == selected_dataset]\n",
    "    print_sample_articles_topic(df_with_topics_selected_technology_dataset, dict_anchor_words, topics, selected_value, size_sample, show_extracts, show_full_text)\n",
    "\n",
    "interact(plot_print_sample_articles_topic, selected_value=[*dict_anchor_words], selected_dataset = [\"TECH\", \"NEWS\", \"ETHICS\", ], size_sample =(5,50, 5), selected_technology=[\"AI\", \"IoT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Impact assessment <a name=\"impact_assessment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The occurence of values can be traced over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6eec6d769648abb9d14947a79e4800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='selected_technology', options=('AI', 'IoT'), value='AI'), Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_create_vis_values_over_time(selected_technology, selected_dataset, resampling, smoothing, max_value_y)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_create_vis_values_over_time (selected_technology, selected_dataset, resampling, smoothing, max_value_y):\n",
    "\n",
    "    T0 = \"1980-01-01\" #YYYY-MM-DD\n",
    "    T1 = \"2023-01-01\" #YYYY-MM-DD\n",
    "\n",
    "    values_to_include_in_visualisation = []\n",
    "    \n",
    "    resampling_dict = {\"Year\": \"Y\", \"Month\": \"M\", \"Day\": \"D\"}\n",
    "    resampling = resampling_dict[resampling]\n",
    "    df_with_topics_short = df_with_topics.loc[(df_with_topics['date'] >= dateutil.parser.parse(T0)) & (df_with_topics['date'] <= dateutil.parser.parse(T1))]\n",
    "    df_with_topics_selected_technology = df_with_topics_short[df_with_topics_short[selected_technology] == True]\n",
    "    df_with_topics_selected_technology_dataset = df_with_topics_selected_technology[df_with_topics_selected_technology['dataset'] == selected_dataset]\n",
    "    create_vis_values_over_time(df_with_topics_selected_technology_dataset, dict_anchor_words, resampling, values_to_include_in_visualisation, smoothing, max_value_y)  \n",
    "    \n",
    "    \n",
    "\n",
    "interact(plot_create_vis_values_over_time, selected_technology=[\"AI\", \"IoT\"], selected_dataset = [\"TECH\", \"NEWS\", \"ETHICS\",], smoothing = (0.25,3, 0.25), max_value_y = (5,100, 5), resampling = [\"Year\", \"Month\", \"Day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ec49412f4f4656addf85367a719d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='selected_value', options=('Justice and Fairness', 'Privacy', 'Cybe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_print_sample_articles_topic(selected_value, size_sample)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_print_sample_articles_topic(selected_value, size_sample):\n",
    "    T0 = \"1960-01-01\" #YYYY-MM-DD\n",
    "    T1 = \"2023-01-01\" #YYYY-MM-DD\n",
    "\n",
    "    show_extracts = True # True, False\n",
    "    show_full_text  = False # True, False\n",
    "\n",
    "    df_with_topics_short = df_with_topics.loc[(df_with_topics['date'] >= dateutil.parser.parse(T0)) & (df_with_topics['date'] <= dateutil.parser.parse(T1))]\n",
    "    print_sample_articles_topic(df_with_topics_short, dict_anchor_words, topics, selected_value, size_sample, show_extracts, show_full_text)\n",
    "\n",
    "interact(plot_print_sample_articles_topic, selected_value=[*dict_anchor_words], size_sample =(5,50, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Values in different societal groups <a name=\"values_in_different_societal_groups\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueMonitor can be used to evaluate which values different societal groups tend to discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d9d80f4c9541fb807eb79a2f963ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='selected_dataset', options=('NEWS', 'ETHICS', 'TECH'), value='NEWS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_values_in_different_groups(selected_dataset)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_values_in_different_groups(selected_dataset):\n",
    "    values_in_different_groups(df_with_topics, dict_anchor_words, selected_dataset)\n",
    "\n",
    "interact(plot_values_in_different_groups, selected_dataset = ['NEWS', 'ETHICS', 'TECH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff98d363eb8247549564468f732a0d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='selected_value', options=('Justice and Fairness', 'Privacy', 'Cybe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_print_sample_articles_topic(selected_value, selected_dataset, size_sample)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_print_sample_articles_topic(selected_value, selected_dataset, size_sample):\n",
    "\n",
    "    show_extracts = True # True, False\n",
    "    show_full_text  = False # True, False\n",
    "\n",
    "    '''--------------------------------------------------------------------------''' \n",
    "\n",
    "    df_with_topics_selected_technology_dataset = df_with_topics[df_with_topics['dataset'] == selected_dataset]\n",
    "    print_sample_articles_topic(df_with_topics_selected_technology_dataset, dict_anchor_words, topics, selected_value, size_sample, show_extracts, show_full_text)\n",
    "interact(plot_print_sample_articles_topic, selected_value=[*dict_anchor_words], selected_dataset = [\"TECH\", \"NEWS\", \"ETHICS\", ], size_sample =(5,50, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BNW2-FjTSaaB",
    "iS91FXNNSmOC",
    "813ejCyOE0iT",
    "EqtK4E3CSnIS",
    "2LlfMeZE7qnp",
    "mgcmijrjXKyh",
    "DbubP0kCVxRf",
    "XZKsT5NR8_xj",
    "qO6wh9EIKu5R",
    "6OuCocWjtKF7",
    "NMD3L0CbQvnt",
    "KUhDIihrA5Tf",
    "n64SVa7SAxDZ",
    "WKtNRP-jBJaI",
    "naoD2x4BBSoI",
    "3m0at_38BNiY",
    "CbX0jG7iBx3f",
    "BDz_NMSQCPkf",
    "4w9uP65Iseb3",
    "a8OYxqoe3xxd",
    "Xx3ok56aJnEM",
    "5F9qr9s72Byr",
    "2A_zQPsW5m_R",
    "cWpwgZcTrIEC"
   ],
   "name": "Topic_modelling_for_value_change.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
